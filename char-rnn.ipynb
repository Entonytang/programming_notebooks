{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hid_size, batch_size, seq_len, n_layers=1):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hid_size = hid_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.gru = nn.GRU(emb_size, hid_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(seq_len * hid_size, vocab_size)\n",
    "        self.selu = nn.SELU()\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "    def forward(self, input, hidden):\n",
    "        x = self.emb(input)\n",
    "        x, hidden = self.gru(x, hidden)\n",
    "        x = x.contiguous().view(self.batch_size, -1)\n",
    "        x = self.selu(self.fc1(x))\n",
    "        x = self.logsoftmax(x)\n",
    "        return x, hidden\n",
    "\n",
    "class CharDataset(data.Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "    def __getitem__(self, index):\n",
    "        inp_seq = self.data[index:(index+self.seq_len-1)]\n",
    "        tgt_seq = torch.Tensor([self.data[index+self.seq_len]]).type(self.data.type())\n",
    "        return inp_seq, tgt_seq\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 25\n",
    "batch_size = 50\n",
    "\n",
    "emb_size = 25\n",
    "hid_size = 100\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394 1115394\n",
      "torch.Size([1115394])\n",
      "22307\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/david/Programming/data/project_gutenberg/tiny-shakespeare.txt\", \"r\") as f:\n",
    "    text_raw = [c for l in f.readlines() for c in l]\n",
    "    #dangling_length = len(text_raw) % seq_length\n",
    "    #text_raw = text_raw[:-dangling_length]\n",
    "\n",
    "charset = sorted(list(set(text_raw)))\n",
    "c2i = {c: i for i, c in enumerate(charset)}\n",
    "i2c = {i: c for c, i in c2i.items()}\n",
    "text_idx = [c2i[c] for c in text_raw]\n",
    "print(len(text_idx), len(text_raw))\n",
    "\n",
    "#inputs = torch.Tensor([x for x in zip(*[text_idx[i::seq_length] for i in range(seq_length-1)])]).long()\n",
    "#targets = torch.Tensor(text_idx[(seq_length-1)::seq_length]).long()\n",
    "inputs = torch.Tensor(text_idx).long()\n",
    "print(inputs.size())\n",
    "\n",
    "#ds = data.TensorDataset(inputs, targets)\n",
    "ds = CharDataset(inputs, seq_length)\n",
    "dl = data.DataLoader(ds, batch_size=batch_size, drop_last=True)\n",
    "print(len(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(charset)\n",
    "num_batches = len(dl)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleGRU (\n",
      "  (emb): Embedding(65, 25)\n",
      "  (gru): GRU(25, 100, batch_first=True)\n",
      "  (fc1): Linear (2400 -> 65)\n",
      "  (selu): SELU\n",
      "  (logsoftmax): LogSoftmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleGRU(vocab_size, emb_size, hid_size, batch_size, seq_length-1, n_layers)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34737486b714652b0ca1bebbbd82500"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-85cc8b15d709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_bar = tqdm_notebook(dl, desc=\"batches\", mininterval=0.9)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i, (mb, tgts) in enumerate(batch_bar):\n",
    "        h = Variable(torch.zeros(n_layers,batch_size, hid_size))\n",
    "        tgts.squeeze_()\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        mb, tgts = Variable(mb), Variable(tgts)\n",
    "        out, h = model(mb, h)\n",
    "        loss = criterion(out, tgts)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        h.detach_()\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 25 == 0 or i == num_batches - 1:\n",
    "            batch_bar.set_postfix(loss=(running_loss / (i+1)))\n",
    "    torch.save(model.state_dict(), \"model_charrnn_{}.pt\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
